{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Neural Style Transfer with PyTorch\n","This notebook implements Neural Style Transfer using PyTorch and a pre-trained VGG19 model. The code will apply the artistic style of one image to the content of another image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"setup_drive"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up Google Drive paths\n","base_path = '/content/drive/MyDrive/Neural_Style_Transfer'\n","content_image_dir = f'{base_path}/content_images'\n","style_image_dir = f'{base_path}/style_images'\n","output_dir = f'{base_path}/output'\n","\n","# Create directories if they don't exist\n","import os\n","for dir_path in [content_image_dir, style_image_dir, output_dir]:\n","    os.makedirs(dir_path, exist_ok=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Install Required Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"install_deps"},"outputs":[],"source":["!pip install torch torchvision pillow imageio"]},{"cell_type":"markdown","metadata":{},"source":["## Import Libraries and Set Up GPU/CPU Device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"import_libs"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from torchvision import transforms, models\n","from PIL import Image\n","import numpy as np\n","import os\n","import imageio\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')"]},{"cell_type":"markdown","metadata":{},"source":["## Define Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"helper_functions"},"outputs":[],"source":["def preprocess_image(image_path, transform):\n","    image = Image.open(image_path).convert('RGB')\n","    return transform(image).unsqueeze(0).to(device)\n","\n","def postprocess(tensor):\n","    image = tensor.to('cpu').clone().squeeze(0)\n","    image = image.detach().numpy().transpose(1, 2, 0)\n","    image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n","    image = np.clip(image, 0, 1)\n","    return (image * 255).astype(np.uint8)\n","\n","def gram_matrix(input_tensor):\n","    batch_size, c, h, w = input_tensor.size()\n","    features = input_tensor.view(batch_size * c, h * w)\n","    G = torch.mm(features, features.t())\n","    return G.div(batch_size * c * h * w)\n","\n","def get_features(image, model, layer_indices):\n","    features = []\n","    x = image\n","    for idx, layer in enumerate(model):\n","        x = layer(x)\n","        if idx in layer_indices:\n","            features.append(x)\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["## Set Up Style Transfer Configuration and Load Images\n","**Note:** Before running this cell, upload your content and style images to the respective folders in Google Drive!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"setup_config"},"outputs":[],"source":["# Configuration\n","content_path = os.path.join(content_image_dir, 'content1.jpeg')\n","style_path = os.path.join(style_image_dir, 'style3.jpeg')\n","iterations = 1000\n","content_weight = 1e2\n","style_weight = 1e7\n","lr = 0.05\n","\n","# Transform\n","transform = transforms.Compose([\n","    transforms.Resize(512),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load images\n","content = preprocess_image(content_path, transform)\n","style = preprocess_image(style_path, transform)\n","\n","# Load VGG19\n","vgg = models.vgg19(weights='IMAGENET1K_V1').features.to(device).eval()\n","for param in vgg.parameters():\n","    param.requires_grad_(False)\n","\n","content_layers = [22]\n","style_layers = [1, 6, 11, 20, 29] # Keep these for now, but consider experimenting\n","\n","content_features = get_features(content, vgg, content_layers)\n","style_features = get_features(style, vgg, style_layers)\n","style_grams = [gram_matrix(f) for f in style_features]\n","\n","# Initial image\n","target = content.clone().requires_grad_(True)\n","optimizer = optim.LBFGS([target], lr=lr)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Run Style Transfer Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"run_optimization"},"outputs":[],"source":["intermediate_images = []\n","iteration = [0]\n","\n","print('Starting optimization...')\n","\n","while iteration[0] <= iterations:\n","    def closure():\n","        optimizer.zero_grad()\n","        target_content = get_features(target, vgg, content_layers)\n","        target_style = get_features(target, vgg, style_layers)\n","        target_grams = [gram_matrix(layer) for layer in target_style]\n","\n","        content_loss = content_weight * torch.mean((target_content[0] - content_features[0])**2)\n","        style_loss = 0\n","        for tg, sg in zip(target_grams, style_grams):\n","            style_loss += torch.mean((tg - sg)**2)\n","        style_loss *= style_weight\n","        total_loss = content_loss + style_loss\n","        total_loss.backward()\n","\n","        if iteration[0] % 100 == 0:\n","            print(f\"Iteration {iteration[0]} | Content Loss: {content_loss.item():.2f}, Style Loss: {style_loss.item():.2f}\")\n","            with torch.no_grad():\n","                img = postprocess(target)\n","                intermediate_images.append(img)\n","                Image.fromarray(img).save(os.path.join(output_dir, f'iter_{iteration[0]}.png'))\n","        iteration[0] += 1\n","        return total_loss\n","    optimizer.step(closure)\n","\n","print('Optimization complete!')"]},{"cell_type":"markdown","metadata":{},"source":["## Save and Display Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"save_results"},"outputs":[],"source":["# Save final image\n","final_image = postprocess(target)\n","Image.fromarray(final_image).save(os.path.join(output_dir, 'final.jpg'))\n","\n","# Save progress GIF\n","if intermediate_images:\n","    imageio.mimsave(os.path.join(output_dir, 'progress.gif'),\n","                    [Image.fromarray(img) for img in intermediate_images], duration=500, loop=0)\n","print(\"Saved final outputs to:\", output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"display_results"},"outputs":[],"source":["# Display the final generated image and progress\n","from IPython.display import Image as IPythonImage\n","\n","# Display the final generated image\n","final_image_path = os.path.join(output_dir, 'final.jpg')\n","display(IPythonImage(filename=final_image_path))\n","\n","# Display the progress GIF\n","if intermediate_images:\n","    progress_gif_path = os.path.join(output_dir, 'progress.gif')\n","    display(IPythonImage(filename=progress_gif_path))"]}],"metadata":{"colab":{"name":"Neural Style Transfer","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}